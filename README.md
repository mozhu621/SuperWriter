<!-- Project Logo (replace with your file in `Fig/` as needed) -->
<p align="center">
  <img src="Fig/SuperWriter.png" width="600px" alt="SuperWriter Logo">
</p>

# ğŸ“š SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models

### Language/è¯­è¨€: [English](./README.md) | [ä¸­æ–‡](./README_zh.md)

<p align="center">
  ğŸ“„ <a href="https://arxiv.org/abs/2506.04180">Paper</a> â€¢ ğŸ¤— <a href="https://huggingface.co/mozhu/SuperWriter-Qwen2.5-7B/tree/main">Model&nbsp;(coming&nbsp;soon)</a>
</p>

**SuperWriter-agent** is an intelligent writing agent framework for **long-form text generation**, inspired by the human workflow of *think first, write later*.  
Through a three-stage **Plan â†’ Write â†’ Refine** process and **Hierarchical DPO** training, the resulting SuperWriter-LM (7 B parameters) matches or surpasses substantially larger models.

---

## ğŸš€ Paper at a Glance
1. **SuperWriter-Agent** explicitly injects *Thinking* and *Reflection* signals.  
2. **Hierarchical DPO + MCTS** back-propagates quality from final outputs.  
3. Scores **8.51 on WritingBench**, ranking 2 nd overallâ€”only behind DeepSeek-R1 (671 B).

---

## ğŸ› ï¸ Method Overview
<p align="center">
  <img src="Fig/Agent (1).png" width="800px" alt="SuperWriter Agent Framework">
</p>

| Stage | Roles / Sub-steps | Goal | Key Mechanisms |
|-------|-------------------|------|----------------|
| **1ï¸âƒ£ Plan**   | *AI Commentators â†” Writer*<br/>Plan Checker | â€¢ Distill topic & structure<br/>â€¢ Produce paragraph-level outline | Story-Workshop dialogue â€¢ Word-budgeting â€¢ Consistency check |
| **2ï¸âƒ£ Write**  | *Thinker â†’ Writer* | â€¢ Draft each section<br/>â€¢ Preserve chapter coherence | **Thinker Step:** bullet ideas & logic â€¢ **Writer Step:** write with previous context |
| **3ï¸âƒ£ Refine** | Checker â†’ Editor | â€¢ Polish draft<br/>â€¢ Improve language & logic | **Checker:** locate weak paragraphs â€¢ **Editor:** targeted rewrite / merge |

### Hierarchical DPO
<p align="center">
  <img src="Fig/DPO (1).png" width="800px" alt="Hierarchical DPO">
</p>

A Monte-Carlo Tree Search builds a three-layer tree *(Plan _i_, Draft _j_, Refine _k_)*.  
Leaf scores are discretized to **+2 â€¦ âˆ’2** and averaged bottom-up to create preference pairs, trained with a single DPO loss.

---

## ğŸ“ˆ Experimental Results

### 1. WritingBench Overall
<p align="center">
  <img src="Fig/WritingBench.png" width="600px" alt="WritingBench Results">
</p>

SuperWriter-LM tops **Academic & Engineering, Finance & Business, Politics & Law, Education** domains and is #1 among models of the same size.

### 2. User Query Win-Rate
<p align="center">
  <img src="Fig/winrate_plots.png" width="600px" alt="Win-Rate Donuts">
</p>

*Scoring*: Win = 1, Tie = 0.5, Loss = 0. Eight donut charts (the 8 th is human evaluation) show SuperWriter-LM dominating the 7 B class and remaining competitive against larger models.

---

## ğŸ§‘â€ğŸ’» Code Guide

### 1. Agent Data Generation
* `Agent/Super_write_agent.py` and `Agent/Super_write_agent_cn.py` generate three-stage (Plan / Write / Refine) SFT data for English and Chinese queries.

### 2. SFT Post-processing
* `Agent/SFT-Process.py` cleans raw agent outputs and produces unified JSONL files.

---

### ğŸ”„ Hierarchical DPO Data Construction
1. Deploy an evaluation service for SFT models (e.g., **SGLang** or a custom HTTP API).  
2. `DPO/MCTS_inference.py` explores Plan â†’ Write â†’ Refine combinations via MCTS.  
3. `DPO/Step_1_query_evaluation_stand.py` creates per-query evaluation rubrics.  
4. `DPO/Step_2_LLM_judge.py` scores all MCTS leaves through the evaluation service.  
5. `DPO/create_dpo_pair.ipynb` selects good vs. bad samples to form final DPO pairs.

---

### â–¶ï¸ Inference Pipeline
Run three successive promptsâ€”Plan, Write, Refineâ€”using the templates in `Inference/superwrite_gen.py` to obtain the final output.

---

### ğŸ‹ï¸â€â™‚ï¸ Training
Fine-tune with **[LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)** and **[360-LLaMA-Factory](https://github.com/Qihoo360/360-LLaMA-Factory)**.  
Many thanks to these projects!

---

## ğŸ¤ Citation
```bibtex
@misc{wu2025superwriterreflectiondrivenlongformgeneration,
      title={SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models}, 
      author={Yuhao Wu and Yushi Bai and Zhiqiang Hu and Juanzi Li and Roy Ka-Wei Lee},
      year={2025},
      eprint={2506.04180},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.04180}, 
}
